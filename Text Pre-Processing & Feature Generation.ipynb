{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import re # regex is imported as it is useful in getting the text that is required and also for tokinzation\n",
    "import requests\n",
    "import io\n",
    "import pdfminer # pdfminer is imported as it is useful for extracting text from the pdf files\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import nltk # nltk is the library for natural laguage processing\n",
    "from nltk.collocations import * # collocations are used to identify the bigrams\n",
    "from itertools import chain\n",
    "import itertools\n",
    "from nltk.tokenize import RegexpTokenizer # tokenize the text using a regular expression\n",
    "from nltk.tokenize import MWETokenizer # MWEtokenizer is for Multi Word Expressions (bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the file links from the pdf document\n",
    "#define function for extracting text because we will be using it often\n",
    "pdfminer_lap = pdfminer.layout.LAParams()\n",
    "setattr(pdfminer_lap, 'all_texts', True)\n",
    "# function to extract text from pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    resource_manager = PDFResourceManager() # creating a resource manager object\n",
    "    file_handle = io.StringIO() \n",
    "    text_converter = TextConverter(resource_manager, file_handle, laparams=pdfminer_lap) # creating a text convertor object\n",
    "    interpreter = PDFPageInterpreter(resource_manager, text_converter) \n",
    "    # opening the pdf file\n",
    "    with open(pdf_path, 'rb') as pdf:\n",
    "        # for loop is used to loop over each page of the pdf to extract the text\n",
    "        for page in PDFPage.get_pages(pdf, caching=True,check_extractable=True):\n",
    "            interpreter.process_page(page)\n",
    "        text = file_handle.getvalue() \n",
    "    # close open handles\n",
    "    text_converter.close()\n",
    "    file_handle.close()\n",
    " \n",
    "    if text:\n",
    "        return text\n",
    "# Using the above function that uses pdfminer, the Group155.pdf file is extracted \n",
    "links_extract = extract_text_from_pdf('inp.pdf')\n",
    "#Use regex to remove unwanted tags and column headers\n",
    "links_extract = re.sub('<.*?>', '', links_extract)\n",
    "links_extract = re.sub('\\\\x0c', '', links_extract)\n",
    "links_extract = re.sub('filename', '', links_extract)\n",
    "links_extract = re.sub('url', '', links_extract)\n",
    "#Get the file names\n",
    "file_name = re.findall('PP[0-9]+',links_extract)\n",
    "#remove extracted and redundant info so that links can be extracted easily\n",
    "links_extract = re.sub('PP[0-9]+.pdf', '', links_extract)\n",
    "links = [] # creating a list\n",
    "for link in links_extract.split('\\n'):\n",
    "    if link != '':\n",
    "        links.append(link) # store all the links in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the pdf files using the links we extracted\n",
    "for index in range(len(links)):\n",
    "    link = links[index] # getting the URL for the file saved in links dictionary\n",
    "    r = requests.get(link, allow_redirects=True)\n",
    "    open(file_name[index] + '.pdf', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the required text  \n",
    "The required text i.e the text in the paper body section of the files is extracted and it is then cleaned by removing the  unnecessary characters like the hexcode, \"\\n\", etc and replacing the ligatures with their ascii counter parts. Ligatures refer to 2 or more characters that are joined into one. The extracted text has some words that have ligatures. Examples of ligatures are 'fi' and 'ff'. Unidecode function is used to decode these ligatures.\n",
    "After cleaning the text, sentence segmentation is applied. Sentence segmentation is the proceess to split text into sentences that end with a '!' or '.' or '?'. NLTK's Punk tokenizer contains a pre-trained sentence tokenizer for English. Hence, this is used to split the text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract text from pdf and store in dictionary with the filename as the key\n",
    "#Use the unidecode package to remove the ligatures i.e to decode the unicode symbols to their closest ascii counter part\n",
    "from unidecode import unidecode\n",
    "body_text = {} # creating a dictionary to store the text from each file\n",
    "# importing the data for nltk's punkt tokenizer for setence segmentation.\n",
    "import nltk.data \n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')# nltk punk tokenizer data is loaded into a variable\n",
    "# Using the for loop to extract the text from each of 200 files, clean the text and store it in a dictionary\n",
    "for index in range(len(links)):\n",
    "    text = extract_text_from_pdf(file_name[index] + '.pdf') \n",
    "    # Using regular expression to extract paper body section of the pdf file      \n",
    "    text = re.search('(?s)Paper Body(.*?)2 References',text).group(1) \n",
    "    # Using Unidecode function from the unidecode library that is imported to remove the ligatures\n",
    "    text = unidecode(text) \n",
    "    # Replacing the words that are within \"<>\" with a space as a part of cleaning the text\n",
    "    text = re.sub('<.*?>', '', text) \n",
    "    # Replacing the hexcode with a space as a part of cleaning\n",
    "    text = re.sub('\\\\x0c', '', text) \n",
    "    # Replacing the words that contain a hyphen and occur at the end of the line\n",
    "    text = re.sub('-\\n','',text) \n",
    "    # Replacing the words that contain a hyphen and have newline character at the end\n",
    "    text = re.sub('-\\n\\n','',text) \n",
    "    temp = []  # creating an empty list\n",
    "    sentences = sent_detector.tokenize(text.strip()) # punkt tokenizer is used on the text that is extracted\n",
    "    # for loop to convert the first letter of the first word of the sentence into lower case    \n",
    "    for sentence in sentences:\n",
    "        temp.append(sentence[0].lower() + sentence[1:]) \n",
    "    text = ' '.join(temp) \n",
    "    # the final list is then stored in the dictionary of body_text for each file\n",
    "    body_text[file_name[index]] = text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the text\n",
    "\n",
    "We are going to break the text down to its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize the text and create another dictinary to store them with the same keys\n",
    "# Using the regular expression given in the assignment question, nltk's tokenizer is used to get the text into tokens\n",
    "tokeniser = RegexpTokenizer(r\"[A-Za-z]\\w+(?:[-'?]\\w+)?\")\n",
    "#create a function to tokenise the text\n",
    "def tokenise(file):\n",
    "    # tokenise the text for each file in stored in the dictionary in the previous cell\n",
    "    tokenised_text = tokeniser.tokenize(body_text[file])\n",
    "    #Return a tuple for saving as dictionary key-value pair\n",
    "    return (file, tokenised_text)\n",
    "# create a dictionary with the tokens for each file\n",
    "text_tokenised = dict(tokenise(file) for file in file_name)\n",
    "# create a list to store all the tokens from all tehe files\n",
    "all_tokens = list(chain.from_iterable(text_tokenised.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams\n",
    "Multiple words that occur together are collocations. Here, bigrams are extracted using nltk functions and then the PMI (Pointwise Mutual Information ) measure is used to identify the commonly occuring bigrams\n",
    "First we extract the common 600 bigrams then we remove the bigrams having stopwords and then using PMI we extract the common 200 bigrams. These words are then added into the vocabulary using MWE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words(all_tokens)\n",
    "token_bigram = finder.nbest(bigram_measures.pmi, 600)\n",
    "# stopwords given in the text file are stored in a list\n",
    "stopwords = open(\"stopwords_en.txt\",'r')\n",
    "stopwords_list = stopwords.readlines()\n",
    "stop_words = []\n",
    "# for loop is used to remove the \"\\n\" character at the end of all stopwords\n",
    "for stopword in stopwords_list:\n",
    "        # stopwords given in the text file are stored in a list\n",
    "        stopword = stopword.rstrip('\\n')\n",
    "        stop_words.append(stopword)\n",
    "        stop_words.append(stopword.capitalize()) # The first letter of the stop word is capitalized and added to the list\n",
    "bigrams = [] # creating a list to store the bigrams with all the stopwords removed\n",
    "# for loop to loop over the bigram collocations that are extracted previously and take only the bigrams without any \n",
    "# stopwords into the bigrams list\n",
    "for token in token_bigram:\n",
    "    # \"(?s)(\\w+)(',)\" regular Expression looks for the first word in the bigram \n",
    "    # and \"(?s)(, ')(\\w+)\" looks for the second word in the bigram. \n",
    "    #The if condition checks if either the first word or the second word of the bigram is in the stopwords   \n",
    "    if ((re.search(\"(?s)(\\w+)(',)\", str(token)).group(1)) or (re.search(\"(?s)(, ')(\\w+)\", str(token)).group(2))) in stop_words:\n",
    "        pass\n",
    "    else: \n",
    "        bigrams.append(token) # if there are no stopwrods in the bigram, add the bigram to the list of bigrams\n",
    "# The above created list of bigrams is used to calculate the 200 most common bigrams using PMI measure       \n",
    "tokens2 = list(chain.from_iterable(bigrams))\n",
    "bigram_measures2 = nltk.collocations.BigramAssocMeasures()\n",
    "finder2 = nltk.collocations.BigramCollocationFinder.from_words(tokens2)\n",
    "token_bigram2 = finder.nbest(bigram_measures2.pmi, 200)\n",
    "# Using MWE tokenizer, the 200 bigrams are added into the vocabulary\n",
    "mwetokenizer = MWETokenizer(token_bigram2)\n",
    "# a dictionary is created and using the MWE tokenizer, the bigrams from each file are stored in it\n",
    "colloc_patents =  dict((pid, mwetokenizer.tokenize(text)) for pid,text in text_tokenised.items())\n",
    "all_words_colloc = list(chain.from_iterable(colloc_patents.values()))\n",
    "colloc_voc = list(set(all_words_colloc))\n",
    "colloc_voc = [w for w in colloc_voc if w not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the stop words (context independent)\n",
    "Removing the words that carry no significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the most common words are found using the FreqDist() method from a set of word tokens.\n",
    "# It gives the frequency distribution based on term frequency i.e number of times the word occured in the entire corpus\n",
    "from nltk.probability import *\n",
    "words_2 = list(chain.from_iterable([set(value) for value in colloc_patents.values()])) # set of word tokens\n",
    "words_2 = [w for w in words_2 if w not in stop_words] # taking the words that are not stop words\n",
    "fd_2 = FreqDist(words_2) # Using freqdist() function to get the frequency of the words\n",
    "y = fd_2.most_common() # most_common() method gives the most frequent words and their frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rare words (3% threshold) and stop words- context dependent (95% threshold)\n",
    "Context dependent stopwords with threshold 95% refers to the words that appear in more than 95% of the documents i.e 95% of 200 which is equal to 190 documents. Rare words with 3% threshold refers to the words that appear in more than 3% of the documents i.e 3% of 200 which is equal to 6. Hence the words that appear in greater than 6 documents and less than 190 documents are useful for us. The other words are not that useful as they may not provide much information about the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rare_plenty = [] # creating an empty list\n",
    "# for loop to loop over the most common words that are extracted in the previous cell\n",
    "for tup in y:\n",
    "# condition to check if the word frequency is within the desired range\n",
    "    if tup[1] > 190 or tup[1] < 6: \n",
    "        rare_plenty.append(tup[0]) # taking all the rare words and stop words in a list\n",
    "colloc_voc = [w for w in colloc_voc if w not in rare_plenty] # if the word is not in the above list, it is added to the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words less than 3 in length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the word length is greater than 3, it is added to the vocabulary\n",
    "colloc_voc = [w for w in colloc_voc if len(w) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary is then sorted in alphabetical order\n",
    "colloc_voc.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the words\n",
    "Stemming reduces the words in different lexical format to its base word. This helps us to get the words with the same root together. For this task, porter stemmer is used below. It is imported from nltk and it is applied on the vocabulary created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "cap_track = []\n",
    "# for loop is used to loop over the words in the vocabulary and extract the words that start with a capital letter\n",
    "for w in colloc_voc:\n",
    "    if w[0].isupper():\n",
    "        cap_track.append(True) # if the word starts with a capital letter, it is added to the list\n",
    "    else:\n",
    "        cap_track.append(False)\n",
    "# for loop is used to loop over the vocabulary to check for bigrams. If the word is not a bigram, it is stemmed and added to\n",
    "# the vocabulary\n",
    "for i in range(len(colloc_voc)):\n",
    "    if '_' in colloc_voc[i]:\n",
    "        continue\n",
    "    else:\n",
    "        colloc_voc[i] = ps.stem(colloc_voc[i])\n",
    "# stemmer lowers the word by default. Hence the words that were previously starting with a capital letter are also in \n",
    "# lowercase by default after stemming. The below code checks if the word was starting with a capital letter by checking the \n",
    "# list that was created above and capitalizes the stemmed word\n",
    "    if cap_track[i] == True:\n",
    "        colloc_voc[i] = colloc_voc[i].capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, stemming  is applied on the dictinary that contains tokens from each file so as to find the frequency of the words in the vocabuary within the documents. This helps in forming the sparse representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop is used to loop over all the files and stem the unigrams found in each file which are then saved in a dictionary\n",
    "for file in file_name:\n",
    "    tokens = colloc_patents[file]\n",
    "    cap_track = []\n",
    "    # for loop is used to loop over the words in the vocabulary and extract the words that start with a capital letter\n",
    "    for w in tokens:\n",
    "        if w[0].isupper():\n",
    "            cap_track.append(True)\n",
    "        else:\n",
    "            cap_track.append(False)\n",
    "    # for loop is used to loop over the vocabulary to check for bigrams. If the word is not a bigram, it is stemmed \n",
    "    #and added tothe vocabulary         \n",
    "    for i in range(len(tokens)):\n",
    "        if '_' in tokens[i]:\n",
    "            continue\n",
    "        else:\n",
    "            tokens[i] = ps.stem(tokens[i]) \n",
    "# stemmer lowers the word by default. Hence the words that were previously starting with a capital letter are also in \n",
    "# lowercase by default after stemming. The below code checks if the word was starting with a capital letter by checking the \n",
    "# list that was created above and capitalizes the stemmed word\n",
    "        if cap_track[i] == True:\n",
    "            tokens[i] = tokens[i].capitalize()\n",
    "    colloc_patents[file] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing is now completed. Every document has to be converted into a numeric representation so that it can be used in the next steps of text mining algorithm. Below we use CountVectorizer that gives a matrix of token counts for a set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vocabulary list with all unique words is created below and it is sorted in alphabetical order\n",
    "colloc_voc=list(set(colloc_voc))\n",
    "colloc_voc.sort()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# CountVectorizer object is created and initialized with our vocabulary\n",
    "vect = CountVectorizer(vocabulary=colloc_voc,lowercase=False) \n",
    "# data frame is created with the file names as index and the words from vocabulary as words\n",
    "df = pd.DataFrame(index=file_name,columns=colloc_voc)\n",
    "# fit_transform method fits the model and learns the vocabulary. It then transforms the text data into feature vectors.\n",
    "# The input to fit_transform has to be a list of strings but we have stored the text as list of tokens, join is used to \n",
    "# concatenate the words and separate them with spaces. \n",
    "for file in file_name:\n",
    "    dtm = vect.fit_transform([' '.join(colloc_patents[file])])\n",
    "    df.loc[file] = dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Task 1\n",
    "2 files have to be generated in task 1. They are the vocab_file and sparse count vectors file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vocab_file is created\n",
    "vocab_file = open('Group155_vocab.txt', 'w+')\n",
    "# for loop is used to loop over the words in the vocabulary to replace the single underscore in the bigrams by \n",
    "# double underscore as per the assignment specification\n",
    "for i in range(len(colloc_voc)):\n",
    "    if '_' in colloc_voc[i]:\n",
    "        colloc_voc[i] = re.sub('_','__',colloc_voc[i])\n",
    "    vocab_file.write(colloc_voc[i] + ':' + str(i) + '\\n') # writing all the words in vocabulary to the file\n",
    "vocab_file.flush() # drain the output buffer\n",
    "vocab_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sparse count vector file is generated\n",
    "count_vec = open('Group155_count_vectors.txt', 'w+')\n",
    "for file in file_name:\n",
    "    count_vec.write(file + ',')\n",
    "    comma = False\n",
    "    for i in range(len(colloc_voc)):\n",
    "        if df.loc[file,colloc_voc[i]] > 0:\n",
    "            count_vec.write(str(i) + ':' + str(df.loc[file,colloc_voc[i]]))\n",
    "            if i != len(colloc_voc)-1:\n",
    "                comma = True\n",
    "        if comma == True:\n",
    "            count_vec.write(',')\n",
    "        comma = False\n",
    "    count_vec.write('\\n')\n",
    "count_vec.flush() # drain the output buffer\n",
    "count_vec.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As per the second task, text for authors, abstracts and titles have to be extracted\n",
    "abstracts ={}\n",
    "titles = {}\n",
    "authors = {}\n",
    "# using for loop to loop over all the files and get the text about authors, abstracts and titles\n",
    "for index in range(len(links)):\n",
    "    text = extract_text_from_pdf(file_name[index] + '.pdf')\n",
    "    text = unidecode(text) # ligatures are removed using the unidecode function\n",
    "    title = re.search('(?s)(.*?)Authored by:',text).group(1) # text for authors is extracted \n",
    "    title = title.lower() # converted to lower case\n",
    "    titles[file_name[index]] = title \n",
    "    abstract = re.search('(?s)Abstract(.*?)1 Paper Body',text).group(1) # text for abstract is extracted\n",
    "    temp = []\n",
    "    # punkt sentence tokenizer is used to convert the first letter of the starting word of a sentence to lower case\n",
    "    sentences = sent_detector.tokenize(abstract.strip())\n",
    "    for sentence in sentences:\n",
    "        temp.append(sentence[0].lower() + sentence[1:])\n",
    "    abstract = ' '.join(temp)\n",
    "    abstracts[file_name[index]] = abstract\n",
    "    author = re.search('(?s)Authored by:(.*?)Abstract',text).group(1)\n",
    "    author = re.findall('[A-Za-z]+ [A-Za-z-]+',author)\n",
    "    authors[file_name[index]] = author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text has to be tokenized for each file and it is then saved in dictionaries for abstracts and titles \n",
    "def tokenise(file,dic):\n",
    "    #tokenise the text\n",
    "    tokenised_text = tokeniser.tokenize(dic[file])\n",
    "    #Return a tuple for saving as dictionary key-value pair\n",
    "    return (file, tokenised_text) \n",
    "abstract_tokenised = dict(tokenise(file,abstracts) for file in file_name) \n",
    "title_tokenised = dict(tokenise(file,titles) for file in file_name)\n",
    "# lists are created to store the tokens from all the files \n",
    "abstract_tokens = list(chain.from_iterable(abstract_tokenised.values()))\n",
    "title_tokens = list(chain.from_iterable(title_tokenised.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Frequently appearing terms in the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show',\n",
       " 'data',\n",
       " 'model',\n",
       " 'paper',\n",
       " 'learning',\n",
       " 'algorithm',\n",
       " 'results',\n",
       " 'based',\n",
       " 'approach',\n",
       " 'problem']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top ten most frequent terms appearing in abstract\n",
    "words_2 = list(chain.from_iterable([set(value) for value in abstract_tokenised.values()]))\n",
    "words_2 = [w for w in words_2 if w not in stop_words] # stop words are removed\n",
    "fd_2 = FreqDist(words_2) # FreqDist method used to get the term frequencies\n",
    "freq_abs = fd_2.most_common() # words with their frequencies are extracted\n",
    "freq_abs = freq_abs[0:10] # top 10 words are taken\n",
    "top_10_abst_terms = []\n",
    "for tup in freq_abs:\n",
    "    top_10_abst_terms.append(tup[0]) #only extract the words\n",
    "top_10_abst_terms #display results \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Frequently appearing terms in the Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'models',\n",
       " 'inference',\n",
       " 'latent',\n",
       " 'gaussian',\n",
       " 'neural',\n",
       " 'optimization',\n",
       " 'process',\n",
       " 'variational',\n",
       " 'networks']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top ten most frequent terms appearing in abstract\n",
    "words_2 = list(chain.from_iterable([set(value) for value in title_tokenised.values()]))\n",
    "words_2 = [w for w in words_2 if w not in stop_words] # stop words are removed\n",
    "fd_2 = FreqDist(words_2) # FreqDist method used to get the term frequencies\n",
    "t_freq = fd_2.most_common() # words with their frequencies are extracted\n",
    "t_freq = t_freq[0:10] # take the top ten\n",
    "top_10_title_terms = []\n",
    "for tup in t_freq:\n",
    "    top_10_title_terms.append(tup[0]) #only extract the words\n",
    "top_10_title_terms #display results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 Most Frequent authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Eric P',\n",
       " 'Francis Bach',\n",
       " 'Alexander G',\n",
       " 'Ambuj Tewari',\n",
       " 'Charles Sutton',\n",
       " 'Dale Schuurmans',\n",
       " 'David Blei',\n",
       " 'David M',\n",
       " 'Devavrat Shah',\n",
       " 'Dustin Tran']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 frequent authors\n",
    "author_tokens = list(chain.from_iterable(authors.values()))\n",
    "from collections import Counter\n",
    "author_count = Counter(author_tokens) \n",
    "top_authors = []\n",
    "l = sorted(author_count.items(), key=lambda x:x[0]) # sort the results by name\n",
    "l = sorted(l, key=lambda x:x[1], reverse = True) # and then by count\n",
    "l = l [0:10]\n",
    "for tup in l:\n",
    "    top_authors.append(tup[0])\n",
    "top_authors #display results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lists of authors, abstracts and titles are written to a dataframe and then this dataframe is converted to a csv file\n",
    "zippedList =  list(zip(top_10_abst_terms,top_10_title_terms,top_authors))\n",
    "csv_df = pd.DataFrame(zippedList, columns = ['top10_terms_in_abstracts','top10_terms_in_titles','top10_authors'])\n",
    "csv_df.to_csv('Group155_stats.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
